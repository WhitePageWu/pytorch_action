{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded4c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9159982f",
   "metadata": {},
   "source": [
    "## tensor的基本属性与生成 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3749e2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.float32\n",
      "tensor([4])\n",
      "torch.Size([2, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#生成2*3的随机张量 \n",
    "a = torch.FloatTensor(2,3)\n",
    "#生成默认类型的2*3的张量通过dtype查看类型\n",
    "b = torch.Tensor(2,3)\n",
    "print(a)\n",
    "print(b.dtype)\n",
    "a = torch.tensor([4])\n",
    "print(a)\n",
    "\n",
    "#生成元素为[4,3]的张量\n",
    "torch.tensor([4,3])\n",
    "#错误写法：\n",
    "#torch.tensor(4,3)\n",
    "#生成符合正态分布的张量，dim1:个数；dim2:通道个数channel；dim3:行长；dim4:列长\n",
    "#生成的张量：若一个维度，一般表示偏置bias；若两个维度，一般表示图片输入[3,28*28]；若三个维度，一般表示文字处理；若四个维度，一般表示图片\n",
    "r = torch.randn(2,3,28,28) \n",
    "print(r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a80901",
   "metadata": {},
   "source": [
    "## 创建Tensor ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f7a49d9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n",
      "[1 2]\n",
      "tensor([1, 2, 3])\n",
      "tensor([1., 2., 3., 4.])\n",
      "tensor([[2.7418e-06, 1.0140e-11, 1.1210e-44],\n",
      "        [0.0000e+00, 1.7136e-07, 1.2779e+22]])\n",
      "tensor([0.0000e+00, 2.5244e-29, 0.0000e+00, 2.5244e-29])\n"
     ]
    }
   ],
   "source": [
    "#从numpy数组中倒入 from_numpy 或者torch.as_tensor()。tensor.numpy()变为numpy数组\n",
    "a = torch.from_numpy(np.array([1,2]))\n",
    "print(a)\n",
    "print(a.numpy())\n",
    "\n",
    "#从python的list数据结构中倒入\n",
    "b = torch.tensor([1,2,3])\n",
    "print(b)\n",
    "#指定类型名创建\n",
    "c = torch.FloatTensor([1,2,3,4])\n",
    "print(c)\n",
    "d = torch.FloatTensor(2,3)\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff92d567",
   "metadata": {},
   "source": [
    "### 总结 ###  \n",
    "\n",
    "1.若用tensor创建。    \n",
    "  torch.tensor((2,4))  #创建包含2，4的tensor  \n",
    "  torch.tensor([2,4])  #创建包含2，4的tensor  \n",
    "  torch.tensor(4)      #创建元素为4的张量 size=[]\n",
    "  torch.tensor([4])    #创建包含元素为4的张来给你 size=[1]  \n",
    "  \n",
    "  \n",
    "2.指定类型创建  \n",
    "  torch.FloatTensor((2,4))  #创建包含2，4的float型tensor   \n",
    "  torch.FloatTensor([2,4])  #创建包含2，4的float型tensor  \n",
    "  torch.FloatTensor(4)      #创建dim1 = 4的float型tensor。即：4个元素一行  \n",
    "  torch.FloatTensor(4,3)    #创建dim1 = 4, dim2 = 3的float型tensor  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0690496",
   "metadata": {},
   "source": [
    "## 初始化tensor ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6d9469a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4313, 0.5069, 0.9568])\n",
      "tensor([[ 0.1360,  1.8631,  0.9356],\n",
      "        [-0.9338, -1.2699, -0.2736],\n",
      "        [ 0.1092, -0.5033,  1.8757]])\n",
      "tensor([[7, 7, 7],\n",
      "        [7, 7, 7],\n",
      "        [7, 7, 7]])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "tensor([9, 3, 6, 7, 0, 1, 2, 5, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "#0-1之间均匀的初始化,生成3*3的tensor\n",
    "a = torch.rand(3,3)\n",
    "print(a)\n",
    "\n",
    "#正态的初始化，生成3*3的tensor\n",
    "b = torch.randn(3,3)\n",
    "print(b)\n",
    "\n",
    "#初始化3*3的tensor，值都为7\n",
    "c = torch.full([3,3],7)\n",
    "print(c)\n",
    "\n",
    "#生成a到b，不包含b的tensor\n",
    "d = torch.arange(0,10)\n",
    "print(d)\n",
    "\n",
    "#eye：值为1的4*4的对角阵；ones：全为1；zeros：全为0\n",
    "e = torch.eye(4)\n",
    "print(e)\n",
    "\n",
    "#生成0-10之间随机的矩阵，并打散\n",
    "f = torch.randperm(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bbabd3",
   "metadata": {},
   "source": [
    "## 维度变化 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "28dae76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "torch.Size([28, 4, 28])\n",
      "tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.]]])\n",
      "torch.Size([4, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 14, 28, 28])\n",
      "torch.Size([16, 1, 392, 392])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([1, 4, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 28, 28, 1])\n"
     ]
    }
   ],
   "source": [
    "#tensor.reshape()\n",
    "a = torch.arange(12.0)\n",
    "a = a.reshape(3,4)\n",
    "print(a)\n",
    "\n",
    "#tensor.view()\n",
    "b = torch.rand(4,1,28,28)\n",
    "print(b.view(28,4,28).shape)\n",
    "\n",
    "#压缩：squeeeze()；展开：unsqueeze()\n",
    "print(a.unsqueeze(0))   #unsqueeze(i),在i后面添加一个维度\n",
    "print(b.squeeze().shape)#squeeze()，移除所有dim=1的维度\n",
    "\n",
    "print(b.shape)\n",
    "print(b.expand(4,14,28,28).shape)  #1->14可行。只有1可以变化\n",
    "print(b.repeat(4,1,14,14).shape)   #各个维度变化倍数\n",
    "\n",
    "#转置transpose(a,b).交换a，b的维度\n",
    "print(b.shape)\n",
    "print(b.transpose(0,1).shape)\n",
    "\n",
    "#permute(0,2,3,1)：dim0不变；dim2->dim1;dim3->dim2;dim1->dim4\n",
    "print(b.shape)\n",
    "print(b.permute(0,2,3,1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c301e7c",
   "metadata": {},
   "source": [
    "## 广播 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ccafdc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 8])\n",
      "torch.Size([4, 32, 8])\n",
      "torch.Size([4, 32, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((4,32,8))\n",
    "b = torch.FloatTensor([5])\n",
    "print((a+b).shape)\n",
    "\n",
    "c = torch.FloatTensor(1,32,1)\n",
    "print((a+c).shape)\n",
    "\n",
    "a = torch.rand((4,32,14,14))\n",
    "b = torch.rand((14,14))\n",
    "print((a+b).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81523ab0",
   "metadata": {},
   "source": [
    "## 合并与切割 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3d2a7ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 32, 8])\n",
      "torch.Size([2, 5, 32, 8])\n",
      "torch.Size([4, 5, 32, 8])\n",
      "torch.Size([3, 5, 32, 8]) torch.Size([1, 5, 32, 8])\n",
      "torch.Size([4, 5, 32, 4]) torch.Size([4, 5, 32, 4])\n"
     ]
    }
   ],
   "source": [
    "#使用cat进行拼接。注意：按dim = d进行拼接时，其他dim的维度必须相同\n",
    "a = torch.rand(4,32,8)\n",
    "b= torch.rand(5,32,8)\n",
    "print(torch.cat([a,b],dim = 0).shape)\n",
    "\n",
    "#使用stack进行拼接，不会将指定维度拼在一起，而是在指定维度之前添加一个维度。0表示原来的a，1表示原来的b。\n",
    "#注意：torch.stack([a,b],dim = d)。 拼接的维度d必须相同。如下都为5\n",
    "a= torch.rand(5,32,8)\n",
    "b= torch.rand(5,32,8)\n",
    "print(torch.stack([a,b],dim = 0).shape)\n",
    "\n",
    "#使用split进行拆分,按长度进行拆分[num1,num2]。拆分为num1和num2的tensor，num1+num2 = 原来的值\n",
    "c = torch.stack([a,b,a,b], dim = 0)\n",
    "print(c.shape)\n",
    "aa1, bb1 = c.split([2,2], dim = 0)\n",
    "aa1.shape,bb1.shape\n",
    "aa2,bb2,cc2 = c.split([2,1,1],dim = 0)\n",
    "aa2.shape,bb2.shape,cc2.shape\n",
    "aa3,bb3 = c.split(3,dim = 0)#拆分为2个,3表示第一个的，第二个的自动推导出\n",
    "print(aa3.shape,bb3.shape)\n",
    "aa4,bb4 = c.chunk(2,dim = 3)#拆分为2个\n",
    "print(aa4.shape,bb4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bcf1a8",
   "metadata": {},
   "source": [
    "### 基本运算 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dd00b9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------加法测试--------\n",
      "tensor([[0.4575, 1.0354, 1.0420],\n",
      "        [0.4469, 1.1079, 1.1655],\n",
      "        [0.8844, 1.1245, 1.6397],\n",
      "        [0.2280, 1.2742, 1.6576]])\n",
      "tensor([[0.4575, 1.0354, 1.0420],\n",
      "        [0.4469, 1.1079, 1.1655],\n",
      "        [0.8844, 1.1245, 1.6397],\n",
      "        [0.2280, 1.2742, 1.6576]])\n",
      "\n",
      "-------减法测试--------\n",
      "tensor([[ 0.2788, -0.2627, -0.3344],\n",
      "        [ 0.2681, -0.1903, -0.2109],\n",
      "        [ 0.7057, -0.1737,  0.2633],\n",
      "        [ 0.0493, -0.0239,  0.2812]])\n",
      "tensor([[ 0.2788, -0.2627, -0.3344],\n",
      "        [ 0.2681, -0.1903, -0.2109],\n",
      "        [ 0.7057, -0.1737,  0.2633],\n",
      "        [ 0.0493, -0.0239,  0.2812]])\n",
      "\n",
      "-------乘法法测试--------\n",
      "tensor([[0.0329, 0.2508, 0.2435],\n",
      "        [0.0319, 0.2978, 0.3285],\n",
      "        [0.0710, 0.3086, 0.6548],\n",
      "        [0.0124, 0.4058, 0.6671]])\n",
      "tensor([[0.0329, 0.2508, 0.2435],\n",
      "        [0.0319, 0.2978, 0.3285],\n",
      "        [0.0710, 0.3086, 0.6548],\n",
      "        [0.0124, 0.4058, 0.6671]])\n",
      "\n",
      "-------除法测试--------\n",
      "tensor([[4.1202, 0.5952, 0.5141],\n",
      "        [4.0007, 0.7069, 0.6935],\n",
      "        [8.8970, 0.7324, 1.3825],\n",
      "        [1.5519, 0.9632, 1.4085]])\n",
      "tensor([[4.1202, 0.5952, 0.5141],\n",
      "        [4.0007, 0.7069, 0.6935],\n",
      "        [8.8970, 0.7324, 1.3825],\n",
      "        [1.5519, 0.9632, 1.4085]])\n",
      "-------矩阵乘法----------\n",
      "a*b =  tensor([[3, 3],\n",
      "        [3, 3]])\n",
      "tensor([[6, 6],\n",
      "        [6, 6]])\n",
      "tensor([[6, 6],\n",
      "        [6, 6]])\n",
      "tensor([[6, 6],\n",
      "        [6, 6]])\n",
      "torch.Size([4, 3, 28, 32])\n",
      "torch.Size([4, 3, 28, 32])\n",
      "tensor(3.)\n",
      "tensor(4.)\n",
      "tensor(3.)\n",
      "tensor(3.)\n",
      "tensor(0.1400)\n",
      "tensor([[11.5540, 12.3727,  0.4324],\n",
      "        [10.1802,  5.8624,  7.2057]])\n",
      "最大值： tensor(12.3727)\n",
      "最小值： tensor(0.4324)\n",
      "均值： tensor(7.9346)\n",
      "方差： tensor(19.7894)\n",
      "对行求和： tensor([24.3591, 23.2484])\n",
      "中位数： tensor(7.2057)\n",
      "将小于10的都变为10： tensor([[11.5540, 12.3727, 10.0000],\n",
      "        [10.1802, 10.0000, 10.0000]])\n",
      "将值限定在0-10之内： tensor([[10., 10.,  8.],\n",
      "        [10.,  8.,  8.]])\n",
      "求最大值的索引位置： tensor(1) ;和0维的最小值的索引位置： tensor([1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(4,3)\n",
    "b = torch.rand(3)\n",
    "\n",
    "print('-------加法测试--------')\n",
    "print(a.add(b))\n",
    "print(a+b)\n",
    "print()\n",
    "print('-------减法测试--------')\n",
    "print(a.sub(b))\n",
    "print(a-b)\n",
    "print()\n",
    "print('-------乘法法测试--------')\n",
    "print(a.mul(b))\n",
    "print(a*b)\n",
    "print()\n",
    "print('-------除法测试--------')\n",
    "print(a.div(b))\n",
    "print(a/b)\n",
    "\n",
    "#矩阵乘法\n",
    "print('-------矩阵乘法----------')\n",
    "a = torch.tensor([[3,3],[3,3]])\n",
    "b = torch.ones(2,2).long()\n",
    "#元素乘法，对应位置相乘\n",
    "print('a*b = ',a*b)\n",
    "#矩阵相乘。1:torch.mm();2:torch.matmul()，相比于mm。此方法可以对高维张量相乘。3:@\n",
    "print(torch.mm(a,b))\n",
    "print(torch.matmul(a,b))\n",
    "print(a@b)\n",
    "#.t()转置\n",
    "a = torch.rand(4,3,28,64)\n",
    "b = torch.rand(4,3,64,32)\n",
    "#print(a.mm(b))#会报错\n",
    "print(torch.matmul(a,b).shape)\n",
    "print((a@b).shape)\n",
    "\n",
    "#平方：a.pow(2) 或者 ** 开方：a.sqrt() 或者 **0.5\n",
    "#torch.exp(a)\n",
    "\n",
    "a = torch.tensor(3.14)\n",
    "print(torch.round(a))#四舍五入\n",
    "print(torch.ceil(a))#上取整\n",
    "print(torch.floor(a))#下取整\n",
    "print(torch.trunc(a))#取整数部分\n",
    "print(torch.frac(a))#取小数部分\n",
    "\n",
    "grad = torch.rand(2,3)*15\n",
    "print(grad)\n",
    "print('最大值：',grad.max())\n",
    "print('最小值：',grad.min())\n",
    "print('均值：',grad.mean())\n",
    "print('方差：',grad.var())\n",
    "print('对行求和：',grad.sum(dim = 1))#0对列求和；1对行求和\n",
    "print('中位数：',grad.median())\n",
    "print('将小于10的都变为10：', grad.clamp(10))\n",
    "print('将值限定在0-10之内：',grad.clamp(8,10))#小于8取8，大于10取10，其他不变\n",
    "print('求最大值的索引位置：',grad.argmax(),';和0维的最小值的索引位置：',grad.argmin(dim = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8383ba8",
   "metadata": {},
   "source": [
    "### 数据统计 ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0c7a095c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.)\n",
      "tensor(2.8284)\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n",
      "tensor([[0.0182, 0.4398, 0.5263, 0.3716, 0.9303],\n",
      "        [0.1398, 0.3868, 0.0902, 0.4231, 0.9096],\n",
      "        [0.9719, 0.9478, 0.2886, 0.5268, 0.3841],\n",
      "        [0.7851, 0.3434, 0.2289, 0.3156, 0.0474]])\n",
      "max:  torch.return_types.max(\n",
      "values=tensor([0.9303, 0.9096, 0.9719, 0.7851]),\n",
      "indices=tensor([4, 4, 0, 0]))\n",
      "argmax:  tensor([4, 4, 0, 0])\n",
      "max,keppdim : torch.return_types.max(\n",
      "values=tensor([[0.9303],\n",
      "        [0.9096],\n",
      "        [0.9719],\n",
      "        [0.7851]]),\n",
      "indices=tensor([[4],\n",
      "        [4],\n",
      "        [0],\n",
      "        [0]]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([[0.9303, 0.5263, 0.4398],\n",
      "        [0.9096, 0.4231, 0.3868],\n",
      "        [0.9719, 0.9478, 0.5268],\n",
      "        [0.7851, 0.3434, 0.3156]]),\n",
      "indices=tensor([[4, 2, 1],\n",
      "        [4, 3, 1],\n",
      "        [0, 1, 3],\n",
      "        [0, 1, 3]]))\n",
      "torch.return_types.kthvalue(\n",
      "values=tensor([0.4398, 0.3868, 0.5268, 0.3156]),\n",
      "indices=tensor([1, 1, 3, 3]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.full([8],1).float()\n",
    "b = a.view(2,4)\n",
    "c = a.view(2,2,2)\n",
    "#norm(p) p范数\n",
    "print(a.norm(1))\n",
    "print(a.norm(2))\n",
    "print(c.norm(2,dim = 1))#在1维度求范数\n",
    "\n",
    "a = torch.rand(4,5)\n",
    "print(a)\n",
    "print('max: ',a.max(dim=1))\n",
    "print('argmax: ',a.argmax(dim = 1))\n",
    "print('max,keppdim :',a.max(dim = 1,keepdim = True))\n",
    "print(a.topk(3,dim=1,largest=True))#返回第1维上最大的3个值,最小设置为False\n",
    "print(a.kthvalue(3,dim = 1,))#返回第1维上第3大的值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e91f995",
   "metadata": {},
   "source": [
    "### 高阶操作 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "106e262b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 3],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "#torch.where(condition,A,B)。\n",
    "a = torch.tensor([[1,2],[3,4]])\n",
    "b = torch.tensor([[1,3],[2,4]])\n",
    "print(torch.where(a>2,a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50a911",
   "metadata": {},
   "source": [
    "### 梯度和激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8e93e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.,  6.],\n",
      "        [ 8., 10.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "x = torch.tensor([[1.0,2.0],[3.0,4.0]],requires_grad = True)\n",
    "y = torch.sum(x**2+2*x+1)\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "a = torch.linspace(-100,100,10)\n",
    "F.tanh(a)\n",
    "torch.tanh(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d519b4c9",
   "metadata": {},
   "source": [
    "### 损失函数一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "40b814e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19.6214)\n",
      "w不需要梯度： tensor([2.])\n",
      "w更新需要梯度后： tensor([2.], requires_grad=True)\n",
      "均方误差： tensor(1., grad_fn=<MseLossBackward>)\n",
      "通过2-范数的平方求误差 tensor(1., grad_fn=<PowBackward0>)\n",
      "test: tensor(1.)\n",
      "tensor([2.])\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "a = torch.linspace(1,10,10)\n",
    "print(torch.norm(a,2))#torch.norm(a,p)。求p范数\n",
    "\n",
    "x = torch.ones(1)\n",
    "w = torch.full([1],2.0)\n",
    "print('w不需要梯度：',w)#w不能求范数\n",
    "w.requires_grad_()#更新w需要梯度\n",
    "print('w更新需要梯度后：',w)\n",
    "\n",
    "#求损失函数方式一：F.mes_loss(predict,label)：均方误差函数。\n",
    "mse = F.mse_loss(torch.ones(1),x*w)#mse_loss = norm(2)**2\n",
    "print('均方误差：',mse)\n",
    "\n",
    "#求损失函数方式二：torch.norm(predict-label)**2\n",
    "print('通过2-范数的平方求误差',torch.norm(torch.ones(1)-x*w,2)**2)\n",
    "\n",
    "print('test:',(torch.ones(1)-x*2).norm(2))\n",
    "\n",
    "#torch.autograd.grad(y,[w,x])：y关于w和x求梯度\n",
    "#print('mes关于w求梯度：',torch.autograd.grad(mse,[w]))\n",
    "\n",
    "#通过loss.backward()求梯度\n",
    "mse.backward()\n",
    "print(w.grad)\n",
    "#通常我们在调试的时候，不会直接看每个变量各个维度的梯度，而是打印改变量的梯度的范数\n",
    "print(w.grad.norm())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0936320e",
   "metadata": {},
   "source": [
    "### 损失函数二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c02df8f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([0.4218, 0.8197, 0.9277], requires_grad=True)\n",
      "tensor([0.2411, 0.3590, 0.3999], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0866,  0.2301, -0.1436]),)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3)\n",
    "\n",
    "a.requires_grad_()\n",
    "p = F.softmax(a, dim = 0)\n",
    "print(a)\n",
    "print(p)\n",
    "#p.backward() #报错,因为p不是一维的。tensor([1])或tensor(1)\n",
    "\n",
    "torch.autograd.grad(p[1],[a],retain_graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f628e26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6931471805599453"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150124f",
   "metadata": {},
   "source": [
    "### 求误差的一个例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8da538dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(64.2846)\n",
      "tensor(64.2846)\n",
      "tensor(64.2846)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "w = torch.rand(32,32)\n",
    "x = torch.rand(32,1)\n",
    "predict = w@x\n",
    "lable = torch.rand(32,1)\n",
    "# print(predict)\n",
    "# print(lable)\n",
    "# print(predict-lable)\n",
    "print(F.mse_loss(predict,lable))#均方误差相对于范数除了一个32，求的是平均\n",
    "print(1/32*torch.norm(predict-lable,2)**2)#采用torch.norm()的方式\n",
    "print(1/32*(predict-lable).norm(2)**2)#采用tensor.norm()的方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c157e02",
   "metadata": {},
   "source": [
    "### BP算法的一个例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e6329e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "误差loss为： tensor(961., grad_fn=<MseLossBackward>)\n",
      "tensor([[1.9375, 1.9375, 1.9375,  ..., 1.9375, 1.9375, 1.9375],\n",
      "        [1.9375, 1.9375, 1.9375,  ..., 1.9375, 1.9375, 1.9375],\n",
      "        [1.9375, 1.9375, 1.9375,  ..., 1.9375, 1.9375, 1.9375],\n",
      "        ...,\n",
      "        [1.9375, 1.9375, 1.9375,  ..., 1.9375, 1.9375, 1.9375],\n",
      "        [1.9375, 1.9375, 1.9375,  ..., 1.9375, 1.9375, 1.9375],\n",
      "        [1.9375, 1.9375, 1.9375,  ..., 1.9375, 1.9375, 1.9375]])\n"
     ]
    }
   ],
   "source": [
    "w = torch.ones(32,32)\n",
    "w.requires_grad_()\n",
    "x = torch.ones(32,1)\n",
    "predict = w@x\n",
    "label = torch.ones(32,1)\n",
    "mse = F.mse_loss(predict,label)\n",
    "print('误差loss为：', mse)\n",
    "\n",
    "#方式一：backward()方法\n",
    "mse.backward()\n",
    "print(w.grad)\n",
    "\n",
    "#方法二：torch.autograd.grad()\n",
    "#print(torch.autograd.grad(mse,[w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cda06b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#二分类\n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=2,alpha=0.25):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha=alpha\n",
    "    def forward(self, input, target):\n",
    "        # input:size is M*2. M is the batch number\n",
    "        # target:size is M.\n",
    "        pt=torch.softmax(input,dim=1)\n",
    "        p=pt[:,1]\n",
    "        loss = -self.alpha*(1-p)**self.gamma*(target*torch.log(p))-\\\n",
    "               (1-self.alpha)*p**self.gamma*((1-target)*torch.log(1-p))\n",
    "        return loss.mean()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5410ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = torch.tensor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
